{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NN_MNIST_IRIS_CIFAR.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNbPlPVGcg5BiSPTtTGBmh3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Abh1-Shek/NeuralNetwork/blob/main/NN_MNIST_IRIS_CIFAR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import sklearn"
      ],
      "metadata": {
        "id": "1X-z7lU6UeVD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = [\n",
        "    [[1, 2, 1], [1, 2, 1]],\n",
        "    [[1, 2, 1], [0, 0, 0]]\n",
        "]\n",
        "b = [\n",
        "    [[1, 2, 1], [1, 2, 1]],\n",
        "    [[0, 0, 0], [1, 2, 1]]\n",
        "]\n",
        "\n",
        "print(np.add(a, b))\n",
        "print(np.subtract(a, b))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hO-2nzjNUsUz",
        "outputId": "304e00eb-a9db-4d0d-a018-ba0f4e2f5c7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[2 4 2]\n",
            "  [2 4 2]]\n",
            "\n",
            " [[1 2 1]\n",
            "  [1 2 1]]]\n",
            "[[[ 0  0  0]\n",
            "  [ 0  0  0]]\n",
            "\n",
            " [[ 1  2  1]\n",
            "  [-1 -2 -1]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class NN:\n",
        "    def __init__(self, sizes):\n",
        "        # suppose sizes = [1, 2, 3]\n",
        "        # then input layer has 1 neuron, hidden 2, and output 3 neurons\n",
        "        self.sizes = sizes\n",
        "        self.num_layers = len(sizes)\n",
        "        \n",
        "        # an array containing the weights matrix for every\n",
        "        # layer excluding the input layer\n",
        "        self.weights = []\n",
        "        for i in range(1, self.num_layers):\n",
        "            self.weights.append(np.random.randn(sizes[i], sizes[i - 1]))\n",
        "            \n",
        "        # an array containing the bias matrices for every layer\n",
        "        # except the input layer\n",
        "        self.biases = [np.random.randn(sz, 1) for sz in sizes[1:]]\n",
        "    \n",
        "    # a function which will calc our output from weights, biases and input\n",
        "    def feed_forward(self, a):\n",
        "        for w, b in zip(self.weights, self.biases):\n",
        "            a = self.sigmoid(np.dot(w, a) + b)\n",
        "        return a\n",
        "    def predict(self, x, single = True):\n",
        "        if single:\n",
        "            return np.argmax(self.feed_forward(x))\n",
        "        else:\n",
        "            res = []\n",
        "            for xi in x:\n",
        "                res.append(self.predict(xi))\n",
        "            return res\n",
        "    \n",
        "    def sigmoid(self, z):\n",
        "        return 1.0 / (1.0 + np.exp(-z))\n",
        "    \n",
        "    def sigmoid_prime(self, z):\n",
        "        return self.sigmoid(z) * (1.0 - self.sigmoid(z))\n",
        "    \n",
        "    # function to calc accuracy (for classification only)\n",
        "    # format = \n",
        "    def evaluate(self, data, labels):\n",
        "        output = [np.argmax(self.feed_forward(x)) for x in data]\n",
        "        labels = [np.argmax(li) for li in labels]\n",
        "        acc = 0\n",
        "        for y_hat, y in zip(output, labels):\n",
        "            acc = acc + int(y_hat == y)\n",
        "            \n",
        "        return (acc * 100) / len(data)\n",
        "    \n",
        "    \n",
        "    # a function which create batches for our SGD to work\n",
        "    def get_batches(self, data, labels, batch_size):\n",
        "        n = len(data)\n",
        "        data, labels = sklearn.utils.shuffle(data, labels)\n",
        "        batches = [\n",
        "            data[i:i + batch_size] for i in range(0, n, batch_size)\n",
        "        ]\n",
        "        batches_labels = [\n",
        "            labels[i:i + batch_size] for i in range(0, n, batch_size)\n",
        "        ]\n",
        "        return batches, batches_labels\n",
        "    \n",
        "    # a function for calling schotastic gradient descent on our data\n",
        "    def SGD(self, data, labels, lr = 0.01, epochs = 10, batch_size = 3, test_data = None, test_labels = None, test = False):\n",
        "        n_test = 0\n",
        "        if test:\n",
        "            n_test = len(test_data)\n",
        "        n = len(data)\n",
        "        \n",
        "        # main algorithm\n",
        "        for epoch in range(epochs):\n",
        "            batches_data, batches_labels = self.get_batches(data, labels, batch_size)\n",
        "            # updating parameters considering every batch\n",
        "            for batch, label in zip(batches_data, batches_labels):\n",
        "                self.update_parameters(batch, label, lr)\n",
        "            \n",
        "            if test:\n",
        "                print (\"Epoch:\", epoch, \"accuracy:\", self.evaluate(test_data, test_labels))\n",
        "            else:\n",
        "                print(\"Epoch:\", epoch, \"complete\")\n",
        "    \n",
        "    def get_next(self, w, activation, b):\n",
        "        z = np.dot(w, activation) + b\n",
        "        return z, self.sigmoid(z)\n",
        "    \n",
        "    # BP1\n",
        "    def get_del_L(self, a_L, y, z_L):\n",
        "        return (a_L - y) * self.sigmoid_prime(z_L)\n",
        "    \n",
        "    # BP2\n",
        "    def get_del_l(self, w_next, del_next, z_l):\n",
        "        return np.dot(w_next.transpose(), del_next) * self.sigmoid_prime(z_l)\n",
        "    \n",
        "    # BP3\n",
        "    def get_del_b(self, del_l):\n",
        "        return del_l\n",
        "    \n",
        "    # BP4\n",
        "    def get_del_w(self, del_l, a_prev):\n",
        "        return np.dot(del_l, a_prev.transpose())\n",
        "    \n",
        "    \n",
        "    def backpropagation(self, X, y):\n",
        "        grad_b = np.array([np.zeros(b.shape) for b in self.biases])\n",
        "        grad_w = np.array([np.zeros(w.shape) for w in self.weights])\n",
        "        \n",
        "        activation = X\n",
        "        activations = [X]\n",
        "        zs = []\n",
        "        \n",
        "        for w, b in zip(self.weights, self.biases):\n",
        "            z, activation = self.get_next(w, activation, b)\n",
        "            zs.append(z)\n",
        "            activations.append(activation)\n",
        "            \n",
        "        del_L = self.get_del_L(activations[-1], y, zs[-1])\n",
        "        \n",
        "        grad_b[-1] = self.get_del_b(del_L)\n",
        "        grad_w[-1] = self.get_del_w(del_L, activations[-2])\n",
        "        \n",
        "        # layers are 0, 1, 2, ..., num_layers - 1\n",
        "        # num_layers - 2 ==> 1\n",
        "        del_l = del_L\n",
        "        for l in range(self.num_layers - 2, 0):\n",
        "            del_l = self.get_del_l(self.weights[l + 1], del_l, zs[l])\n",
        "            grad_b[l] = self.get_del_b(del_l)\n",
        "            grad_w[l] = self.get_del_w(del_l, activations[l - 1])\n",
        "        \n",
        "        return (grad_w, grad_b)\n",
        "        \n",
        "        \n",
        "    # def update_parameters(self, batch, label, lr):\n",
        "    #     for i in range(len(batch)):\n",
        "    #         del_w, del_b = self.backpropagation(batch[i], label[i])\n",
        "            \n",
        "    #         self.weights = np.subtract(self.weights, lr * del_w / len(batch))\n",
        "    #         self.biases = np.subtract(self.biases, lr * del_b / len(batch))\n",
        "    \n",
        "    def update_parameters(self, batch, label, lr):\n",
        "        grad_w = [np.zeros(w.shape) for w in self.weights]\n",
        "        grad_b = [np.zeros(b.shape) for b in self.biases]\n",
        "        \n",
        "        for i in range(len(batch)):\n",
        "            del_w, del_b = self.backpropagation(batch[i], label[i])\n",
        "            grad_w = np.add(grad_w, del_w)\n",
        "            grad_b = np.add(grad_b, del_b)\n",
        "        \n",
        "        self.weights = np.subtract(self.weights, lr * grad_w / len(batch))\n",
        "        self.biases = np.subtract(self.biases, lr * grad_b / len(batch))\n",
        "            "
      ],
      "metadata": {
        "id": "PyZuuFiOUt5w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "myNN = NN([4, 20, 3])"
      ],
      "metadata": {
        "id": "gOg8yAW0UwEV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn import datasets"
      ],
      "metadata": {
        "id": "rZfEiP6pUyx_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iris = datasets.load_iris()\n",
        "print(iris.keys())\n",
        "print(iris.target_names)\n",
        "print(iris.feature_names)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M863tMpaU0Qs",
        "outputId": "2d7b7bd4-bd28-4cb0-db56-865bc5309da0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names', 'filename', 'data_module'])\n",
            "['setosa' 'versicolor' 'virginica']\n",
            "['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocessing(dataset):\n",
        "    DS = dataset.data\n",
        "    scaler = MinMaxScaler()\n",
        "    DS = scaler.fit_transform(DS)\n",
        "    y = []\n",
        "    yo = [\n",
        "        [[1], [0], [0]], \n",
        "        [[0], [1], [0]], \n",
        "        [[0], [0], [1]]\n",
        "    ]\n",
        "    X = []\n",
        "    nf = len(DS[0])\n",
        "    for i in range(len(DS)):\n",
        "        X.append(np.resize(DS[i], (nf, 1)))\n",
        "    for yi in dataset.target:\n",
        "        y.append(yo[yi])\n",
        "    return np.array(X), np.array(y)"
      ],
      "metadata": {
        "id": "hzZd1vRJU1p_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = preprocessing(iris)\n",
        "print(X.shape)\n",
        "print(y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EISE-_XaU3Og",
        "outputId": "2328c2e1-75e4-46a4-983f-57fbe2f06193"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(150, 4, 1)\n",
            "(150, 3, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "myNN.SGD(X, y, 0.1, 5000, 50, X, y, True)"
      ],
      "metadata": {
        "id": "eTkQaOlwU4bJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for w, b in zip(myNN.weights, myNN.biases):\n",
        "    print('w', w.shape)\n",
        "    print('b', b.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hr3FwAZSU6La",
        "outputId": "d2c47d24-484c-4503-9d0f-da1a626b87d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "w (20, 4)\n",
            "b (20, 1)\n",
            "w (3, 20)\n",
            "b (3, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "res = myNN.predict(X, False)"
      ],
      "metadata": {
        "id": "dqzm7LiKU_9q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sum(res == iris.target) / len(res))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0UyU5TfGVBbi",
        "outputId": "6d6f6d64-746c-4e66-afeb-8ed021540578"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9266666666666666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(res)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HhvcIfJJVC1E",
        "outputId": "671fa50e-657a-4191-f0f2-5940b1c94d38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets"
      ],
      "metadata": {
        "id": "4ai4g0fsVEJo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "digits = datasets.load_digits()\n",
        "print(digits.data.shape)\n",
        "print(digits.target)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YDCJDeBnVFpa",
        "outputId": "e86c1e44-a5cd-4d47-aed4-d727f755184a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1797, 64)\n",
            "[0 1 2 ... 8 9 8]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocessing_mnist(dataset):\n",
        "    DS = dataset.data\n",
        "    scaler = MinMaxScaler()\n",
        "    DS = scaler.fit_transform(DS)\n",
        "    X = []\n",
        "    y = []\n",
        "    nf = len(DS[0])\n",
        "    num_classes = 10\n",
        "    yo = [\n",
        "        np.zeros((num_classes, 1)) for i in range(num_classes)\n",
        "    ]\n",
        "    for i in range(num_classes):\n",
        "        yo[i][i][0] = 1\n",
        "    # see preprocessing function for looking at the structure of yo\n",
        "    for i in range(len(DS)):\n",
        "        #(64,) => (64,1)  #because we need vector\n",
        "        X.append(np.resize(DS[i], (nf, 1)))\n",
        "        y.append(yo[dataset.target[i]])\n",
        "        \n",
        "    return np.array(X), np.array(y)"
      ],
      "metadata": {
        "id": "5Q_rhaX0VG4h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_mnist, y_mnist = preprocessing_mnist(digits)"
      ],
      "metadata": {
        "id": "BuUIuwxAVIVb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NN_mnist = NN([64, 32, 10])"
      ],
      "metadata": {
        "id": "zyp0E9chVJ1s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NN_mnist.SGD(X_mnist, y_mnist, \n",
        "             lr = 0.5, epochs = 5000,\n",
        "             batch_size = 100, \n",
        "             test_data = X_mnist, test_labels = y_mnist, test = True)"
      ],
      "metadata": {
        "id": "1Z9NDZSJVP80"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions_mnist = NN_mnist.predict(X_mnist, False)"
      ],
      "metadata": {
        "id": "7jF6kcmTVRg1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sum(predictions_mnist == digits.target) / len(predictions_mnist) * 100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R56wPOwhVdg3",
        "outputId": "6c75b18c-9cef-4063-8828-e7c6dbcc27f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10.072342793544797\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow\n",
        "from tensorflow.keras.datasets.cifar10 import load_data as cifar"
      ],
      "metadata": {
        "id": "OYPmzopHVsM5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train, y_train), (x_test, y_test) = cifar()"
      ],
      "metadata": {
        "id": "D64CkVJZV8L7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocessing_cifar10(X_, y_):\n",
        "    X_ = np.array(X_)\n",
        "    X = []\n",
        "    y = []\n",
        "    nf1 = len(X_[0])\n",
        "    nf2 = len(X_[0][0])\n",
        "    nf3 = len(X_[0][0][0])\n",
        "\n",
        "    scaler = MinMaxScaler()\n",
        "\n",
        "\n",
        "    # nd = len(X_)\n",
        "    nd = 3000\n",
        "    num_classes = 10\n",
        "    yo = [np.zeros((num_classes, 1)) for i in range(num_classes)]\n",
        "    for i in range(num_classes):\n",
        "        yo[i][i][0] = 1\n",
        "    \n",
        "    for i in range(nd):\n",
        "        X.append(np.resize(X_[i], (nf1 * nf2 * nf3, 1)))\n",
        "        X[i] = X[i] / 255\n",
        "        y.append(yo[y_[i][0]])\n",
        "\n",
        "    return np.array(X), np.array(y)"
      ],
      "metadata": {
        "id": "LUJTffthV9XX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = preprocessing_cifar10(x_train, y_train)\n",
        "X_val, y_val = preprocessing_cifar10(x_test, y_test)"
      ],
      "metadata": {
        "id": "Au7CI9q1Wojq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X.shape)\n",
        "print(y.shape)\n",
        "# print(y[0:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dCXgeMHJXHam",
        "outputId": "aa1bfe78-22ed-4699-8bee-322517368135"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3000, 3072, 1)\n",
            "(3000, 10, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "NN_cifar = NN([3072, 256, 128, 10])"
      ],
      "metadata": {
        "id": "Ju0bHLfCY5KI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NN_cifar.SGD(X, y,\n",
        "             lr = 1, epochs = 10,\n",
        "             batch_size = 1, \n",
        "             test_data = X, test_labels = y, test = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ZahEducZBIB",
        "outputId": "5267eb58-71ba-4c92-cc30-7575b589f138"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:103: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:104: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:144: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:145: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 accuracy: 9.133333333333333\n",
            "Epoch: 1 accuracy: 9.133333333333333\n",
            "Epoch: 2 accuracy: 9.7\n",
            "Epoch: 3 accuracy: 10.533333333333333\n",
            "Epoch: 4 accuracy: 10.566666666666666\n",
            "Epoch: 5 accuracy: 10.966666666666667\n",
            "Epoch: 6 accuracy: 12.033333333333333\n",
            "Epoch: 7 accuracy: 10.8\n",
            "Epoch: 8 accuracy: 12.5\n",
            "Epoch: 9 accuracy: 11.566666666666666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "e2pbxbMoZepd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}